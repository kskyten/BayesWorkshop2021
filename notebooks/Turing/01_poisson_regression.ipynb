{
  "cells": [
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using DrWatson\n@quickactivate \"BayesWorkshop2021\"\ninclude(joinpath(srcdir(), \"setup.jl\"));"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The problem\n\n### Background\n\nImagine that you are a statistician or data scientist working as an independent\ncontractor. One of your clients is a company that owns many residential buildings\nthroughout New York City. The property manager explains that they are concerned about the number\nof cockroach complaints that they receive from their buildings. Previously\nthe company has offered monthly visits from a pest inspector as a solution to\nthis problem. While this is the default solution of many property managers in\nNYC, the tenants are rarely home when the inspector visits, and so the manager\nreasons that this is a relatively expensive solution that is currently not very\neffective.\n\nOne alternative to this problem is to deploy long term bait stations. In this\nalternative, child and pet safe bait stations are installed throughout the\napartment building. Cockroaches obtain quick acting poison from these stations\nand distribute it throughout the colony. The manufacturer of these bait stations\nprovides some indication of the space-to-bait efficacy, but the manager suspects\nthat this guidance was not calculated with NYC roaches in mind. NYC roaches, the\nmanager rationalizes, have more hustle than traditional roaches; and NYC\nbuildings are built differently than other common residential buildings in the\nUS. This is particularly important as the unit cost for each bait station per\nyear is quite high.\n\n### The goal\n\nThe manager wishes to employ your services to help them to find the optimal\nnumber of roach bait stations they should place in each of their buildings in\norder to minimize the number of cockroach complaints while also keeping\nexpenditure on pest control affordable.\n\nA subset of the company's buildings have been randomly selected for an experiment:\n\n* At the beginning of each month, a pest inspector randomly places a number of\nbait stations throughout the building, without knowledge of the current\ncockroach levels in the building\n* At the end of the month, the manager records\nthe total number of cockroach complaints in that building.\n* The manager would like to determine the optimal number of traps ($\\textrm{traps}$) that\nbalances the lost revenue ($R$) that complaints ($\\textrm{complaints}$) generate\nwith the all-in cost of maintaining the traps ($\\textrm{TC}$).\n\nFortunately, Bayesian data analysis provides a coherent framework for us to tackle this problem.\n\nFormally, we are interested in finding\n\n$$\n\\arg\\max_{\\textrm{traps} \\in \\mathbb{N}} \\mathbb{E}_{\\text{complaints}}[R(\\textrm{complaints}(\\textrm{traps})) - \\textrm{TC}(\\textrm{traps})]\n$$\n\nThe property manager would also, if possible, like to learn how these results\ngeneralize to buildings they haven't treated so they can understand the\npotential costs of pest control at buildings they are acquiring as well as for\nthe rest of their building portfolio.\n\nAs the property manager has complete control over the number of traps set, the\nrandom variable contributing to this expectation is the number of complaints\ngiven the number of traps. We will model the number of complaints as a function\nof the number of traps.\n\n## The data\n\nThe data provided to us is in a file called `pest_data.feather`. Let's\nload the data and see what the structure is:\n\nWe have access to the following fields:\n\n* `complaints`: Number of complaints per building per month\n* `building_id`: The unique building identifier\n* `traps`: The number of traps used per month per building\n* `date`: The date at which the number of complaints are recorded\n* `live_in_super`: An indicator for whether the building as a live-in super\n* `age_of_building`: The age of the building\n* `total_sq_foot`: The total square footage of the building\n* `average_tenant_age`: The average age of the tenants per building\n* `monthly_average_rent`: The average monthly rent per building\n* `floors`: The number of floors per building\n\nFirst, let's see how many buildings we have data for:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "n_buildings = length(unique(pest_data.building_id))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "And make some plots of the raw data:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using VegaLite\n\npest_data |>\n@vlplot(\n  :bar,\n  x = :complaints\n)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "pest_data |>\n@vlplot(\n  :point,\n  x = :traps,\n  y = :complaints,\n  color = \"live_in_super:N\"\n)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "pest_data |>\n@vlplot(\n  :point,\n  x = :date,\n  y = :complaints,\n  color = \"live_in_super:N\"\n)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using RCall\nggplot = rimport(\"ggplot2\")\n\nggplot.ggplot(pest_data, ggplot.aes(x = :complaints)) + ggplot.geom_bar()"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ggplot.ggplot(pest_data, ggplot.aes(x = :traps, y = :complaints, color = :live_in_super)) + ggplot.geom_jitter()"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first question we'll look at is just whether the number of complaints per\nbuilding per month is associated with the number of bait stations per building\nper month, ignoring the temporal and across-building variation (we'll come back\nto those sources of variation later in the document). That requires only two\nvariables, $\\textrm{complaints}$ and $\\textrm{traps}$. How should we model the\nnumber of complaints?\n\n\n## Bayesian workflow\n\nSee slides\n\n## Modeling count data : Poisson distribution\n\nWe already know some rudimentary information about what we should expect. The\nnumber of complaints over a month should be either zero or an integer. The\nproperty manager tells us that it is possible but unlikely that number of\ncomplaints in a given month is zero. Occasionally there are a very large number\nof complaints in a single month. A common way of modeling this sort of skewed,\nsingle bounded count data is as a Poisson random variable. One concern about\nmodeling the outcome variable as Poisson is that the data may be\nover-dispersed, but we'll start with the Poisson model and then check\nwhether over-dispersion is a problem by comparing our model's predictions\nto the data.\n\n### Model\n\nGiven that we have chosen a Poisson regression, we define the likelihood to be\nthe Poisson probability mass function over the number bait stations placed in\nthe building, denoted below as `traps`. This model assumes that the mean and\nvariance of the outcome variable `complaints` (number of complaints) is the\nsame. We'll investigate whether this is a good assumption after we fit the\nmodel.\n\nFor building $b = 1,\\dots,10$ at time (month) $t = 1,\\dots,12$, we have\n\n$$\n\\begin{align*}\n\\textrm{complaints}_{b,t} & \\sim \\textrm{Poisson}(\\lambda_{b,t}) \\\\\n\\lambda_{b,t} & = \\exp{(\\eta_{b,t})} \\\\\n\\eta_{b,t} &= \\alpha + \\beta \\, \\textrm{traps}_{b,t}\n\\end{align*}\n$$\n\nLet's encode this probability model in a Turing model.\n\n### Writing our first model"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Turing\n\n@model function simple_poisson(; traps, complaints=missing)\n    # TODO: Implement the prior!\n\n    # This will be compiled away by Julia's compiler, i.e.\n    # it has zero cost at runtime. It allows us to use `.~`\n    # both to sample and observe.\n    if complaints === missing\n        complaints = Vector{Int}(undef, length(traps))\n    end\n\n    @. complaints ~ Poisson(exp(α + β * traps))\n\n    return (; α, β, complaints)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making sure our code is right\n\nHowever, before we fit the model to real data, we should check that our\nmodel works well with simulated data. We'll simulate data according to the model\nand then check that we can sufficiently recover the parameter values used\nin the simulation. We can simulate the data by calling the `rand()` method on\nour model."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "N = size(pest_data, 1)\nmean_traps = mean(pest_data.traps)\nfake_traps = rand(filldist(Poisson(mean_traps), N));"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit the model to the fake data:\nA Turing `Model` is really just a function, hence we can get a sample \nof the return-values under the prior, which in our case is all the parameters in the model,\nby simply calling the instantiated model. Hence we just instantiate the model\non the `fake_traps` and call it:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "fake_model = simple_poisson(traps=fake_traps);\nfake_data = fake_model();\nfake_complaints = fake_data.complaints;\nfake_data"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the simulated data we fit the model to see if we can recover\nthe `alpha` and `beta` parameters used in the simulation.\n\n### Assess parameter recovery\n\nTo condition `simple_poisson` we simply override the `missing` default value for `complaints`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "fake_post = sample(simple_poisson(traps=fake_traps, complaints=fake_complaints), NUTS(), 1_000)\nmcmc_recover_hist(fake_post, (α = fake_data.α, β = fake_data.β))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't do a great job recovering the parameters here simply because we're\nsimulating so few observations that the posterior uncertainty remains rather\nlarge, but it looks at least _plausible_ ($\\alpha$ and $\\beta$ are contained\nwithin the histograms). If we did the simulation with many more observations the\nparameters would be estimated much more precisely.\n\nTo get a better sense of the fit it's useful to look at the in-sample predictions.\nWe simply re-instantiate the model but now with `complaints=missing` again, and then\nsample from the model _conditioned_ on the chain `fake_post` using `predict`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Note: `posterior_predictive` is just a wrapper around `Turing.predict`\n# so we instead end up working with a `Vector{<:NamedTuple}` instead of\n# a `MCMCChains.Chains`. This is to stay consistent with the other tutorials\n# e.g. Soss.jl's tutorial.\nyrep = posterior_predictive(simple_poisson(traps=fake_traps), fake_post)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a plot of the density estimate of the observed data compared to\n200 of the `yrep` datasets:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ppc_dens_overlay(fake_complaints, yrep[1:200])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the plot above we have the kernel density estimate of the observed data ($y$,\nthicker curve) and 200 simulated data sets ($y_{rep}$, thin curves) from the\nposterior predictive distribution. If the model fits the data well, as it does\nhere, there is little difference between the observed dataset and the simulated\ndatasets.\n\nAnother plot we can make for count data is a rootogram. This is a plot of the\nexpected counts (continuous line) vs the observed counts (blue histogram). We\ncan see the model fits well because the observed histogram matches the expected\ncounts relatively well."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ppc_rootogram(fake_complaints, yrep[1:200])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit with real data\n\nTo fit the model to the actual observed data, again, we just set the `complaints` to\nthe observations and `sample`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "post = sample(simple_poisson(traps=pest_data.traps, complaints=pest_data.complaints), NUTS(), 1_000)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also plot the posterior distributions:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using StatsPlots\nplot(post)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we expected, it appears the number of bait stations set in a building is\nassociated with the number of complaints about cockroaches that were made in the\nfollowing month. However, we still need to consider how well the model fits the data.\n\n\n### Posterior predictive checking\n\nLet's produce some samples of `complaints` given the chain `post`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "yrep = posterior_predictive(simple_poisson(traps=pest_data.traps), post)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ppc_dens_overlay(pest_data.complaints, yrep[1:200])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As opposed to when we fit the model to simulated data above, here the simulated\ndatasets is not as dispersed as the observed data and don't seem to capture the\nrate of zeros in the observed data. The Poisson model may not be sufficient for\nthis data.\n\nLet's explore this further by looking directly at the proportion of zeros in the\nreal data and predicted data."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ppc_stat(pest_data.complaints, yrep, stat = R\"function(x) mean(x == 0)\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot above shows the observed proportion of zeros (thick vertical line) and\na histogram of the proportion of zeros in each of the simulated data sets. It is\nclear that the model does not capture this feature of the data well at all.\n\nThis next plot is a plot of the standardised residuals of the observed vs predicted number of complaints."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Convert into a matrix of size `(num_variables, num_samples)`\nyrep_matrix = mapreduce(hcat, yrep) do y\n    y.complaints\nend;\n\nqqnorm(dropdims(mean(yrep_matrix; dims=2); dims=2), qqline=:R)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see here, it looks as though we have more positive residuals than negative,\nwhich indicates that the model tends to underestimate the number of complaints\nthat will be received.\n\nThe rootogram is another useful plot to compare the observed vs expected number\nof complaints. This is a plot of the expected counts (continuous line) vs the\nobserved counts (blue histogram):"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ppc_rootogram(pest_data.complaints, yrep)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the model was fitting well these would be relatively similar, however in this\nfigure we can see the number of complaints is underestimated if there are few\ncomplaints, over-estimated for medium numbers of complaints, and underestimated\nif there are a large number of complaints.\n\nWe can also view how the predicted number of complaints varies with the number\nof traps. From this we can see that the model doesn't seem to fully capture the\ndata."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ppc_intervals(pest_data.complaints, yrep, x = pest_data.traps) + ggplot.labs(x = \"Number of traps\", y = \"Number of complaints\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifically, the model doesn't capture the tails of the observed data very\nwell.\n\n### ArviZ.jl\n\nThe above closely follows the original Stan tutorial, but often it will be more convenient to reach for other Julia packages to perform visualization and posterior analysis. In particular, [ArviZ.jl](https://github.com/arviz-devs/ArviZ.jl) provides a signficant toolbox for doing posterior analysis and model comparison, e.g. some of the plots from above becomes a simple function call. Throughout we'll therefore make use of following `to_arviz` method:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using ArviZ, LinearAlgebra\n\n# https://arviz-devs.github.io/ArviZ.jl/stable/quickstart/#Convert-to-InferenceData\nfunction to_arviz(prior_model::Turing.Model, conditioned_model::Turing.Model, posterior_samples::MCMCChains.Chains)\n    # Sample from the prior of the latent variables, i.e. everything\n    # except `complaints`.\n    prior_samples = sample(conditioned_model, Prior(), 1_000, progress=false);\n    # Sample \"predictions\" of `complaints` under `prior_samples`.\n    prior_predictive = predict(prior_model, prior_samples);\n    # Sample predictions of `complains` under `posterior_samples`.\n    posterior_predictive = predict(prior_model, posterior_samples);\n    # Compute p(constraints[i] ∣ params...) where `params` are given\n    # by samples form `posterior_samples`.\n    loglikelihoods = Turing.pointwise_loglikelihoods(\n        conditioned_model,\n        MCMCChains.get_sections(posterior_samples, :parameters)\n    );\n\n    # Ensure the ordering of the loglikelihoods matches the ordering of `posterior_predictive`.\n    ynames = string.(keys(posterior_predictive));\n    loglikelihoods_vals = getindex.(Ref(loglikelihoods), ynames);\n    # Reshape into `(nchains, nsamples, size(y)...)`\n    loglikelihoods_arr = permutedims(cat(loglikelihoods_vals...; dims=3), (2, 1, 3));\n\n    # Finally convert into `InferenceData`.\n    return from_mcmcchains(\n        posterior_samples;\n        posterior_predictive=posterior_predictive,\n        log_likelihood=Dict(\"complaints\" => loglikelihoods_arr),\n        prior=prior_samples,\n        prior_predictive=prior_predictive,\n        observed_data=Dict(\"complaints\" => conditioned_model.args.complaints),\n        library=\"Turing\",\n    )\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is in fact included in `setup.jl` but we also include it here for demonstration purposes. This means that we can now do:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "idata = to_arviz(\n    simple_poisson(traps=pest_data.traps),\n    simple_poisson(traps=pest_data.traps, complaints=pest_data.complaints),\n    post\n)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "giving us an `InferenceData` instance, which in turn gives us access to ArviZ's toolbox, e.g. we can compare the prior distribution and the posterior distribution to see if we actually learned anything by calling `plot_dist_comparison`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plot_dist_comparison(idata)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hey, we did! Or equivalent of the `ppc_dens_overlay` from above:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plot_ppc(idata)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be using more of ArviZ's functionality later on."
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.6.2"
    },
    "kernelspec": {
      "name": "julia-1.6",
      "display_name": "Julia 1.6.2",
      "language": "julia"
    }
  },
  "nbformat": 4
}
