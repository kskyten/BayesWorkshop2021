{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling joint distributions using Turing\n",
    "\n",
    "The goal of Bayesian data analysis is to come up with a plausible explanation for some phenomenon or data in the form of a joint distribution. Defining complicated multivariate distributions outright is extremely difficult, so statistician typically decompose the task into defining conditional distributions and place independence assumptions on the interactions between the random variables.\n",
    "\n",
    "To get a proper introduction to Turing.jl, its syntax and everything surrounding the ecosystem, see [https://turing.ml/](https://turing.ml/stable/).\n",
    "\n",
    "# Model syntax\n",
    "\n",
    "Turing syntax is designed to closely resemble the typical mathematical notation for Bayesian models.\n",
    "\n",
    "## Model definition\n",
    "\n",
    "### `@model`\n",
    "\n",
    "A Turing model is defined by using the model macro as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T15:51:32.328000+01:00",
     "start_time": "2021-07-26T14:51:06.496Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "demo (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Turing\n",
    "\n",
    "@model function demo(y; σ²=1.0)\n",
    "    n=length(y)\n",
    "\n",
    "    σ² ~ InverseGamma(2, 3)\n",
    "    σ = √σ²\n",
    "    x ~ Normal(0, σ)\n",
    "\n",
    "    y ~ MvNormal(x * ones(n), 1.0)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the right-hand side of `~` is expected to be a `Distribution` from [Distributions.jl](https://github.com/JuliaStats/Distributions.jl).\n",
    "\n",
    "Notice that there's seemingly nothing special about the body of the function above, with the exception of the `~` statements! In fact, _any_ Julia code can go into a Turing model, e.g. [solving differential equations](https://turing.ml/dev/tutorials/10-bayesian-differential-equations/)! \n",
    "\n",
    "All that `@model` does is to replace the `~` statements with some function calls and it adds some additional arguments to the function that Turing uses internally to keep track of everything. If you're very keen on understanding some of its internals, check out [https://turing.ml/dev/docs/for-developers/compiler](https://turing.ml/dev/docs/for-developers/compiler).\n",
    "\n",
    "A Turing model is then instantiated with some data, e.g. `demo(randn(10))`, to condition the model on said data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T15:52:24.606000+01:00",
     "start_time": "2021-07-26T14:52:22.295Z"
    }
   },
   "outputs": [],
   "source": [
    "m = demo(ones(10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample from the `Model` we simply call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T15:52:28.096000+01:00",
     "start_time": "2021-07-26T14:52:24.688Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Float64}:\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the observations was returned back to us for the model call. This is not because Turing has some feature that it always returns the observations or anything, no no no, it's simply that `y ~ ...` was the last statement in the model, hence it was returned. This is exactly the same behavior as a standard Julia function! Often one might want to make the return value something different, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T15:52:29.025000+01:00",
     "start_time": "2021-07-26T14:52:26.380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(σ = 1.0, x = 0.7551512526930272, y = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], whatever = [100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function demo(y; σ²=1.0)\n",
    "    n = length(y)\n",
    "\n",
    "    σ² ~ InverseGamma(2, 3)\n",
    "    σ = √σ²\n",
    "    x ~ Normal(0, σ)\n",
    "\n",
    "    y ~ MvNormal(x * ones(n), 1.0)\n",
    "\n",
    "    return (; σ, x, y, whatever=100 .* y)\n",
    "end\n",
    "\n",
    "m = demo(ones(10));\n",
    "m()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be extremely useful for debugging but also if there are certain quantities one might want to compute from the model. As we'll see later, there's a `generated_quantities` function that allows you to get the return-values of the model _conditioned_ on posterior samples.\n",
    "\n",
    "### Usage of `missing`: sampling observations\n",
    "\n",
    "We can also instantiate `demo` _without_ any observations by simply setting `y` to `missing`, and in Turing a `missing` variable is considered as a random variable to be sampled rather than an observation. \n",
    "\n",
    "But note when we do this for the above `demo` model, the default `n = length(y)` doesn't make sense anymore since `length(::Missing)` isn't defined, i.e. we need to specify the length of the variable `y` when it's `missing`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T15:52:30.214000+01:00",
     "start_time": "2021-07-26T14:52:27.563Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(σ = 1.0, x = -0.9470887353923412, y = [-0.4966704092922676, -0.9802262472224543, -2.0684170042881345, -1.308281109013812, -0.38466325572132387, -0.16784824343988825, -2.6450263246008276, 0.6694487817739021, -0.7887400747487461, -2.9013218108240055], whatever = [-49.66704092922676, -98.02262472224544, -206.84170042881345, -130.8281109013812, -38.46632557213239, -16.784824343988824, -264.50263246008274, 66.94487817739021, -78.8740074748746, -290.13218108240056])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If `y` is not an array, we default to using `n=1`.\n",
    "@model function demo(y; σ²=1.0, n = y isa AbstractArray ? length(y) : 1)\n",
    "    σ² ~ InverseGamma(2, 3)\n",
    "    σ = √σ²\n",
    "    x ~ Normal(0, σ)\n",
    "\n",
    "    y ~ MvNormal(x * ones(n), 1.0)\n",
    "\n",
    "    return (; σ, x, y, whatever=100 .* y)\n",
    "end\n",
    "\n",
    "m = demo(missing, n=10)\n",
    "m()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for the keyword argument `σ²`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T15:52:30.850000+01:00",
     "start_time": "2021-07-26T14:52:28.580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(σ = 1.1503460317662495, x = 1.402065757613783, y = [1.0173912416143962, 1.645529225907439, 0.7607617591417798, 0.007716147223924219, -1.1481330756305723, 2.5297646933134494, 2.6536747465136488, -0.49334734068235697, 1.4875121678687626, 1.8834481053204142], whatever = [101.73912416143962, 164.5529225907439, 76.07617591417798, 0.7716147223924219, -114.81330756305724, 252.97646933134496, 265.3674746513649, -49.3347340682357, 148.75121678687626, 188.3448105320414])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = demo(missing; σ² = missing, n=10)\n",
    "m()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loops\n",
    "\n",
    "To model a random vector, where the components depend on the index, we use `for` loops. In Turing this is just the standard Julia code, but you need to remember to allocate an array to store the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T15:52:41.634000+01:00",
     "start_time": "2021-07-26T14:52:41.572Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "demo (generic function with 3 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function demo(::Type{TV} = Vector{Float64}) where {TV}\n",
    "    y = TV(undef, J)\n",
    "    for j = 1:J\n",
    "        Normal(θ[j], σ[j])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding the type-parameter `TV` we allow Turing to internally replace `TV` with types that are compatible with whatever AD framework we're using at the time, e.g. if we're using ForwardDiff.jl (which is the case by default), `TV` is replaced by `Vector{ForwardDiff.Dual{Float64}}`. One could of course just do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T15:52:43.225000+01:00",
     "start_time": "2021-07-26T14:52:43.200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "demo (generic function with 3 methods)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function demo()\n",
    "    y = Vector(undef, J)\n",
    "    for j = 1:J\n",
    "        Normal(θ[j], σ[j])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but this would lead to type-unstable code and hence a loss in performance.\n",
    "\n",
    "## Broadcasted statements, i.e. `.~`\n",
    "\n",
    "In Turing you can also use the broadcasted version of `~` where the semantics are the same as the broadcasting behavior in Julia, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T15:52:44.545000+01:00",
     "start_time": "2021-07-26T14:52:44.515Z"
    }
   },
   "outputs": [],
   "source": [
    "x .~ Normal(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T15:52:33.725000+01:00",
     "start_time": "2021-07-26T14:52:30.923Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in eachindex(x)\n",
    "    x[i] ~ Normal(0, 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "are equivalent. Similarly, if `μ` is a vector, the following are equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T15:52:33.755000+01:00",
     "start_time": "2021-07-26T14:52:31.588Z"
    }
   },
   "outputs": [],
   "source": [
    "x .~ Normal.(μ, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in eachindex(x)\n",
    "    x[i] ~ Normal(μ[i], 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that one can also use the `@. expr...` macro from Julia which tells Turing to broadcast everything following `@.`, i.e. the following is equivalent to the above snippets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@. x ~ Normal(μ, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is something we'll use repeatedly throughout as, amongst other things, `.~` provides a convenient way of expressing a `Vector` of IID samples.\n",
    "\n",
    "### Sampling with `.~`\n",
    "\n",
    "One drawback with the `x .~ dist` is that it requires the left-hand side `x` to be defined, i.e. `x` either needs to be part of the arguments to the model and is thus considered an observation (unless it's `missing`), _or_ we need to allocate an `Array` to hold the result similar to what we did for for-loops earlier. Therefore we'll often see the following piece of code being used throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T15:53:43.388000+01:00",
     "start_time": "2021-07-26T14:53:43.341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "demo (generic function with 3 methods)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function demo(n; x=missing)\n",
    "    # If `x` is `missing`, we consider it as a random variable to be sampled.\n",
    "    if x === missing\n",
    "        # Since trying to do `x[i] ~ ...` doesn't make sense when `x === missing`\n",
    "        # we need to replace `x` with an `Array` that can hold the results.\n",
    "        x = Vector(undef, n)\n",
    "    end\n",
    "\n",
    "    x .~ Normal()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above in the section on for-loops, `Vector(undef, n)` will be type-unstable, and so the above should really be written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T15:53:44.951000+01:00",
     "start_time": "2021-07-26T14:53:44.384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "demo (generic function with 4 methods)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function demo(n, ::Type{T}; x=missing) where {T}\n",
    "    # If `x` is `missing`, we consider it as a random variable to be sampled.\n",
    "    if x === missing\n",
    "        # Since trying to do `x[i] ~ ...` doesn't make sense when `x === missing`\n",
    "        # we need to replace `x` with an `Array` that can hold the results.\n",
    "        x = Vector{T}(undef, n)\n",
    "    end\n",
    "\n",
    "    x .~ Normal()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for optimal performance.\n",
    "\n",
    "We can also make the statement a bit more compact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T15:53:46.257000+01:00",
     "start_time": "2021-07-26T14:53:46.230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "demo (generic function with 4 methods)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function demo(n, ::Type{T}; x=missing) where {T}\n",
    "    x = x === missing ? Vector{T}(undef, n) : x\n",
    "    x .~ Normal()\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
